{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk import word_tokenize\n",
    "from nltk import SyllableTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, LassoCV, SGDRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix,classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay, mean_squared_error, classification_report\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "stopwords_spacy = list(nlp.Defaults.stop_words)\n",
    "stopwords_nltk = nltk.corpus.stopwords.words('spanish')\n",
    "morestpwords = list(set(stopwords_spacy) - set(stopwords_nltk) )\n",
    "STOPWORDS = stopwords_nltk + morestpwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "* Extracción de características\n",
    "    * Textuales\n",
    "        * Número de caracteres por documento\n",
    "        * Número de dígitos\n",
    "        * Número de palabras por documento\n",
    "        * Número de caracteres por palabra\n",
    "        * Numero de mayúsculas utilizadas por documento\n",
    "        * Número de caracteres especiales\n",
    "        * Número de emoticones (:),:/, <3 etc\n",
    "        * Numero de emojis\n",
    "        * FKGL\n",
    "    * Características semánticas\n",
    "        * Número de verbos\n",
    "        * Número de adjetivos\n",
    "        * Número de sustantivos\n",
    "        * Número de pronombres\n",
    "    * TfidfVectorizer\n",
    "        * BoW\n",
    "        * Bigramas de palabras\n",
    "        * Bigramas de etiquetas POS\n",
    "\n",
    "\n",
    "* Entrenamiento del Modelo      \n",
    "    * Stratified K Fold\n",
    "    * Logistic Regression\n",
    "    * Random Forest\n",
    "\n",
    "\n",
    "\n",
    "* Validación del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de funciones para extraer características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add must be an array \n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add)], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Características del texto\n",
    "def numcaracteres(docs):\n",
    "    len_caracteres = []\n",
    "    for doc in docs:\n",
    "        raw = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', doc)\n",
    "        raw = emoji.replace_emoji(raw, replace = '').lower().strip()\n",
    "        raw = re.sub('_', ' ', raw.lower().strip())\n",
    "        raw = re.sub('\\n', ' ', raw.lower().strip())\n",
    "        raw = re.sub(' {2,}', ' ', raw.lower().strip())\n",
    "        len_caracteres.append(len(raw))\n",
    "    len_caracteres = np.array(len_caracteres).reshape(-1, 1)\n",
    "    return len_caracteres\n",
    "def numdigitos(docs):\n",
    "    len_digitos = np.array([len(re.findall('\\d', doc)) for doc in docs]).reshape(-1, 1)\n",
    "    return len_digitos\n",
    "def palsxdoc(docs):\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    palsperdoc =[]\n",
    "    for doc in docs:\n",
    "        raw = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', doc)\n",
    "        raw = emoji.replace_emoji(raw, replace = '').lower().strip()\n",
    "        raw = re.sub('_', ' ', raw.lower().strip())\n",
    "        raw = re.sub('\\n', ' ', raw.lower().strip())\n",
    "        raw = re.sub(' {2,}', ' ', raw.lower().strip())\n",
    "        palsperdoc.append(len(nlp(raw)))\n",
    "        #palsperdoc = np.array([len(nltk.word_tokenize(re.sub('[^\\w\\s]|\\d]', '', doc.lower()))) for doc in docs]).reshape(-1, 1)\n",
    "    return np.array(palsperdoc).reshape(-1, 1)\n",
    "def charsxpal(docs):\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    tokens =[]\n",
    "    for doc in docs:\n",
    "        raw = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', doc)\n",
    "        raw = emoji.replace_emoji(raw, replace = '').lower().strip()\n",
    "        raw = re.sub('_', ' ', raw.lower().strip())\n",
    "        raw = re.sub('\\n', ' ', raw.lower().strip())\n",
    "        raw = re.sub(' {2,}', ' ', raw.lower().strip())\n",
    "        tokens.append(nlp(raw))\n",
    "    #tokens = [nltk.word_tokenize(re.sub('[^\\w\\s]|\\d', '', doc.lower())) for doc in docs]\n",
    "    chars_per_token = []\n",
    "    for i in range(len(tokens)):\n",
    "        toks_p = []\n",
    "        if len(tokens[i]) == 0:\n",
    "            chars_per_token.append(0)\n",
    "        else:\n",
    "            for token in tokens[i]:\n",
    "                toks_p.append(len(token))\n",
    "            chars_per_token.append(np.mean(toks_p))\n",
    "    chars_per_token = np.array(chars_per_token).reshape(-1, 1)                        \n",
    "    return chars_per_token\n",
    "def UpperCase_doc(docs): ### MEJORAR CONTANDO EL TOTAL DE PALABRAS COMPLETAMENTE EN MAYÚSCULAS\n",
    "    upper_cnt = np.array([len(re.findall('[A-Z]', doc)) for doc in docs]).reshape(-1, 1)\n",
    "    return upper_cnt\n",
    "def UpperCase_compl(docs):\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    palsperdoc =[]\n",
    "    for doc in docs:\n",
    "        raw = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', doc)\n",
    "        raw = emoji.replace_emoji(raw, replace = '').strip()\n",
    "        raw = re.sub('_', ' ', raw.strip())\n",
    "        raw = re.sub('\\n', ' ', raw.strip())\n",
    "        raw = re.sub(' {2,}', ' ', raw.strip())\n",
    "        if len(nlp(raw)) == 0:\n",
    "            palsperdoc.append(0)\n",
    "        else:\n",
    "            palsperdoc.append(sum([token.text.isupper() for token in nlp(raw)])/ len(nlp(raw)))\n",
    "    palsperdoc = np.array(palsperdoc).reshape(-1, 1)\n",
    "    return \n",
    "def specchar(docs): ### MEJORAR SEPARAR ENTRE COMAS ADMIRACIÓN PARENTÉSIS PUNTOS RISAS JAJAJA HAHAHA LOL----Pa después\n",
    "    speccharcnt = np.array([len(re.findall('[^\\w\\s]', doc)) for doc in docs]).reshape(-1, 1)\n",
    "    return speccharcnt\n",
    "def cntemojis(docs):\n",
    "    emojiscnt = np.array([emoji.emoji_count(doc) for doc in docs]).reshape(-1, 1)\n",
    "    return emojiscnt\n",
    "def LexRich(docs): ### MEJORAR ESTA PARTE DE LA RIQUEZA LEXICA HACIENDO LEMATIZACIÓN-- MEJORADO\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    LexRichperdoc = []\n",
    "    for doc in docs:\n",
    "        raw = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', doc)\n",
    "        raw = emoji.replace_emoji(raw, replace = '').lower().strip()\n",
    "        raw = re.sub('_', ' ', raw.lower().strip())\n",
    "        raw = re.sub('\\n', ' ', raw.lower().strip())\n",
    "        raw = re.sub(' {2,}', ' ', raw.lower().strip())\n",
    "        if len(nlp(raw)) == 0:\n",
    "            LexRichperdoc.append(0)\n",
    "        else:\n",
    "            LexRichperdoc.append(len(set([token.lemma_ for token in nlp(raw)]))/len(nlp(raw)))\n",
    "        #np.array([len(set(nltk.word_tokenize(re.sub('[^\\w\\s]|\\d', '', doc.lower()))))/ len(nltk.word_tokenize(re.sub('[^\\w\\s]|\\d', '', doc.lower()))) for doc in docs]).reshape(-1, 1)\n",
    "    LexRichperdoc = np.array(LexRichperdoc).reshape(-1, 1)\n",
    "    return LexRichperdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKGL(docs): #### HACER LA TOKENIZACIÓN MÁS LAS SENTENCIAS CON SPACY\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    def fkgl(doc):\n",
    "        text = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', doc)\n",
    "        text = emoji.replace_emoji(text, replace = '').lower().strip()\n",
    "        text = re.sub('_', ' ', text.lower().strip())\n",
    "        text = re.sub('\\n', ' ', text.lower().strip())\n",
    "        text = re.sub(' {2,}', ' ', text.lower().strip())\n",
    "        tk = SyllableTokenizer()\n",
    "        words = [token.text for token in nlp(text)]\n",
    "        sentences = [sent for sent in nlp(doc).sents]\n",
    "        silabas = tk.tokenize(text)\n",
    "        if len(words) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            fkgl_doc = 206.84 - 1.02 * (len(words)/len(sentences)) - 60 * (len(silabas)/len(words))\n",
    "            return fkgl_doc\n",
    "    FKGL_perdoc = np.array([fkgl(doc) for doc in docs]).reshape(-1, 1)\n",
    "    return FKGL_perdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Caracteristicas semánicas  ## HACER EL RATIO LAS CARACTERÍSTICAS/#TOTAL DE PALABRAS\n",
    "def POS_Vect(docs): ### Devuelve VERB,ADJ, NOUN, PRON y POS_xTweet\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    POS_tags = []\n",
    "    for i in docs:\n",
    "        text = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', i)\n",
    "        text = emoji.replace_emoji(text, replace = '').lower().strip()\n",
    "        text = re.sub('_', ' ', text.lower().strip())\n",
    "        text = re.sub('\\n', ' ', text.lower().strip())\n",
    "        text = re.sub(' {2,}', ' ', text.lower().strip())\n",
    "        doc = nlp(text)\n",
    "        POS_tags.append([token.pos_ for token in doc])\n",
    "        \n",
    "    POS_tweets = [' '.join(i) for i in POS_tags]\n",
    "    \n",
    "    VERBS = np.array([i.count('VERB') for i in POS_tags]).reshape(-1, 1)\n",
    "    ADJS = np.array([i.count('ADJ') for i in POS_tags]).reshape(-1, 1)\n",
    "    NOUNS = np.array([i.count('NOUN') for i in POS_tags]).reshape(-1, 1)\n",
    "    PRONS = np.array([i.count('PRON') for i in POS_tags]).reshape(-1, 1)\n",
    "    \n",
    "    return [VERBS, ADJS, NOUNS, PRONS, POS_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(docs):\n",
    "    vect_bow = CountVectorizer(stop_words = STOPWORDS)\n",
    "    X_vect_bow = vect_bow.fit_transform(docs)\n",
    "    return X_vect_bow\n",
    "def BigramWord(docs):\n",
    "    bigram_vect = CountVectorizer(stop_words = STOPWORDS, ngram_range=(2, 2))\n",
    "    X_bigram_words = bigram_vect.fit_transform(docs)\n",
    "    return X_bigram_words\n",
    "def TfidfBoW(docs):\n",
    "    vect_TfidfBoW = TfidfVectorizer(stop_words = STOPWORDS)\n",
    "    X_TfidfBoW = vect_TfidfBoW.fit_transform(docs)\n",
    "    return X_TfidfBoW\n",
    "def TfidfBigram(docs):\n",
    "    bigram_vectfidf = TfidfVectorizer(stop_words = STOPWORDS, ngram_range=(2, 2))\n",
    "    X_bigram_tfidf = bigram_vectfidf.fit_transform(docs)\n",
    "    return X_bigram_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet</th>\n",
       "      <th>humor</th>\n",
       "      <th>prejudice_woman</th>\n",
       "      <th>prejudice_lgbtiq</th>\n",
       "      <th>prejudice_inmigrant_race</th>\n",
       "      <th>gordofobia</th>\n",
       "      <th>mean_prejudice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72157</td>\n",
       "      <td>Mi celular tiene una aplicación que te hace ve...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68084</td>\n",
       "      <td>En esta vida me tocó tener mala suerte, espero...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69089</td>\n",
       "      <td>Tu mamá es taaan taan obesa, que cuando pasa f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69190</td>\n",
       "      <td>Mi tía me dijo: \\n- tengo memoria de Elefante....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70474</td>\n",
       "      <td>- Mamá, en el colegio me dicen gorda.\\n- ¡Ay M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              tweet  humor  \\\n",
       "0  72157  Mi celular tiene una aplicación que te hace ve...      1   \n",
       "1  68084  En esta vida me tocó tener mala suerte, espero...      1   \n",
       "2  69089  Tu mamá es taaan taan obesa, que cuando pasa f...      1   \n",
       "3  69190  Mi tía me dijo: \\n- tengo memoria de Elefante....      1   \n",
       "4  70474  - Mamá, en el colegio me dicen gorda.\\n- ¡Ay M...      1   \n",
       "\n",
       "   prejudice_woman  prejudice_lgbtiq  prejudice_inmigrant_race  gordofobia  \\\n",
       "0                0                 0                         0           1   \n",
       "1                0                 0                         0           1   \n",
       "2                0                 0                         0           1   \n",
       "3                0                 0                         0           1   \n",
       "4                0                 0                         0           1   \n",
       "\n",
       "   mean_prejudice  \n",
       "0             3.0  \n",
       "1             2.8  \n",
       "2             3.6  \n",
       "3             3.4  \n",
       "4             3.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('train.csv')\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_train.tweet, data_train.humor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lendoc = numcaracteres(X)\n",
    "numdigitos_ = numdigitos(X)\n",
    "palsdoc = palsxdoc(X)\n",
    "charspal = charsxpal(X)\n",
    "mayusdoc = UpperCase_doc(X)\n",
    "speccharcnt = specchar(X)\n",
    "emojiscnt = cntemojis(X)\n",
    "LexRich_ = LexRich(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ا'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ذ'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ه'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ب'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'إ'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ل'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ى'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'د'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ك'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'م'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ر'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ة'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'و'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ح'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'أ'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ä'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ª'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ń'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "flskGL = FKGL(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBcnt, ADJcnt, NOUNcnt, PRONcnt, POSxtweet = POS_Vect(X)\n",
    "X_bow_vect = BoW(X)\n",
    "bigram_BoW = BigramWord(X)\n",
    "bigram_POS = BigramWord(POSxtweet)\n",
    "X_TfidfBoW = TfidfBoW(X)\n",
    "bigram_TfidfBoW = TfidfBigram(X)\n",
    "bigram_TfidfPOS = TfidfBigram(POSxtweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [lendoc, numdigitos_,palsdoc,charspal,mayusdoc,speccharcnt,emojiscnt,\n",
    "            LexRich_,flskGL,VERBcnt,ADJcnt,NOUNcnt,PRONcnt]\n",
    "ft_idx = ['lendoc', 'numdigitos_','palsdoc','charspal','mayusdoc','speccharcnt','emojiscnt',\n",
    "          'LexRich_','flskGL','VERBcnt','ADJcnt','NOUNcnt','PRONcnt']\n",
    "Bows = [X_bow_vect,bigram_BoW,bigram_POS,X_TfidfBoW,bigram_TfidfBoW,bigram_TfidfPOS]\n",
    "Bows_idx = ['X_bow_vect_train','bigram_BoW_train','bigram_POS_train','X_TfidfBoW_train','bigram_TfidfBoW_train',\n",
    "            'bigram_TfidfPOS_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_ftures = []\n",
    "z = add_feature(features[0], features[1])\n",
    "Combined_ftures.append(z)\n",
    "for i in range(2,13):\n",
    "    z = add_feature(z, features[i])\n",
    "    Combined_ftures.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('Features_train.npz', Combined_ftures[-1])\n",
    "for i in range(len(Bows)):\n",
    "    scipy.sparse.save_npz(Bows_idx[i] +'.npz', Bows[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_test.tweet\n",
    "y_test = data_test.humor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_bow = CountVectorizer(stop_words = STOPWORDS)\n",
    "X_vect_bow = vect_bow.fit_transform(X)\n",
    "\n",
    "bigram_vect = CountVectorizer(stop_words = STOPWORDS, ngram_range=(2, 2))\n",
    "X_bigram_POS = bigram_vect.fit_transform(POSxtweet)\n",
    "\n",
    "vect_bibow = CountVectorizer(stop_words = STOPWORDS, ngram_range=(2, 2))\n",
    "X_vect_bibow = vect_bibow.fit_transform(X)\n",
    "\n",
    "vect_TfidfBoW = TfidfVectorizer(stop_words = STOPWORDS)\n",
    "X_TfidfBoW = vect_TfidfBoW.fit_transform(X)\n",
    "\n",
    "bigram_vectfidf = TfidfVectorizer(stop_words = STOPWORDS, ngram_range=(2, 2))\n",
    "bigram_tfidf_POS = bigram_vectfidf.fit_transform(POSxtweet)\n",
    "\n",
    "bigram_vectfidf_bow = TfidfVectorizer(stop_words = STOPWORDS, ngram_range=(2, 2))\n",
    "bigram_tfid = bigram_vectfidf_bow.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'á'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: ' '\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'í'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'é'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ñ'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ó'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: ' '\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ü'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ú'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angel\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ø'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lendoc_test = numcaracteres(X_test)\n",
    "numdigitos_test = numdigitos(X_test)\n",
    "palsdoc_test = palsxdoc(X_test)\n",
    "charspal_test = charsxpal(X_test)\n",
    "mayusdoc_test = UpperCase_doc(X_test)\n",
    "speccharcnt_test = specchar(X_test)\n",
    "emojiscnt_test = cntemojis(X_test)\n",
    "LexRich_test = LexRich(X_test)\n",
    "flskGL_test = FKGL(X_test)\n",
    "VERBcnt_test, ADJcnt_test, NOUNcnt_test, PRONcnt_test, POSxtweet_test = POS_Vect(X_test)\n",
    "X_bow_vect_test = vect_bow.transform(X_test)\n",
    "bigram_BoW_test = vect_bibow.transform(X_test) #####\n",
    "bigram_POS_test = bigram_vect.transform(POSxtweet_test)\n",
    "X_tfidfbow_test = vect_bow.transform(X_test)\n",
    "bigram_TfidfBoW_test = bigram_vectfidf_bow.transform(X_test)#####\n",
    "bigram_tfidfPOS_test = bigram_vect.transform(POSxtweet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = [lendoc_test, numdigitos_test, palsdoc_test, charspal_test, mayusdoc_test, \n",
    "                 speccharcnt_test, emojiscnt_test, LexRich_test,flskGL_test,VERBcnt_test,ADJcnt_test,NOUNcnt_test,\n",
    "                 PRONcnt_test]\n",
    "\n",
    "ft_idx_test = ['lendoc_test', 'numdigitos_test','palsdoc_test','charspal_test','mayusdoc_test',\n",
    "               'speccharcnt_test','emojiscnt_test','LexRich_test','flskGL_test','VERBcnt_test','ADJcnt_test',\n",
    "               'NOUNcnt_test','PRONcnt_test']\n",
    "Bows_test = [X_bow_vect_test,bigram_BoW_test,bigram_POS_test,X_tfidfbow_test,bigram_TfidfBoW_test,bigram_tfidfPOS_test]\n",
    "Bows_test_idx = ['X_bow_vect_test','bigram_BoW_test','bigram_POS_test','X_tfidfbow_test','bigram_TfidfBoW_test','bigram_tfidfPOS_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_ftures_tst = []\n",
    "z = add_feature(features_test[0], features_test[1])\n",
    "Combined_ftures_tst.append(z)\n",
    "for i in range(2,13):\n",
    "    z = add_feature(z, features_test[i])\n",
    "    Combined_ftures_tst.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('Features_test.npz', Combined_ftures_tst[-1])\n",
    "for i in range(len(Bows_test)):\n",
    "    scipy.sparse.save_npz(Bows_test_idx[i] +'.npz', Bows_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento \n",
    "\n",
    "    * StratifiedKFold y LogisticRegression\n",
    "    * StratifiedKFold y SVM\n",
    "    * StratifiedKFold y RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfLR = LogisticRegression(solver = 'lbfgs', tol = 0.001, C = 0.01, class_weight = 'balanced')\n",
    "clfSVC = SVC(C = 0.01, kernel = 'linear', class_weight = 'balanced')\n",
    "clfRF = RandomForestClassifier(max_depth = 10, random_state = 0, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resultados(X,y,clf):\n",
    "    skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "    lst_f1_stratified = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        x_train_fold, x_test_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "        clf.fit(x_train_fold, y_train_fold)\n",
    "        lst_f1_stratified.append(f1_score(clf.predict(x_test_fold), y_test_fold))\n",
    "    return(np.mean(lst_f1_stratified))\n",
    "    #print(f'Promedio F1: {np.mean(lst_f1_stratified)}')\n",
    "    #print(f'Varianza F1: {np.std(lst_f1_stratified)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empezamos a probar con cada uno de los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [lendoc, numdigitos_,palsdoc,charspal,mayusdoc,speccharcnt,emojiscnt,LexRich_,flskGL,VERBcnt,ADJcnt,NOUNcnt,PRONcnt,\n",
    "            X_bow_vect,bigram_BoW,bigram_POS,X_TfidfBoW,bigram_TfidfBoW,bigram_TfidfPOS]\n",
    "ft_idx = ['lendoc', 'numdigitos_','palsdoc','charspal','mayusdoc', 'PalMayus','speccharcnt','emojiscnt','LexRich_','flskGL','VERBcnt','ADJcnt',\n",
    "          'NOUNcnt','PRONcnt', 'X_bow_vect','bigram_BoW','bigram_POS','X_TfidfBoW','bigram_TfidfBoW','bigram_TfidfPOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_per_ft = []\n",
    "for i in range(len(features)):\n",
    "    LR_per_ft.append((Resultados(features[i], y, clfLR), ft_idx[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(LR_per_ft, key=lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_per_ft = []\n",
    "for i in range(len(features)):\n",
    "    SVM_per_ft.append((Resultados(features[i], y, clfSVC), ft_idx[i]))\n",
    "sorted(SVM_per_ft, key=lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_per_ft = []\n",
    "for i in range(len(features)):\n",
    "    RF_per_ft.append((Resultados(features[i], y, clfRF), ft_idx[i]))\n",
    "sorted(RF_per_ft, key=lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados combinando las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAScaler = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ftures_LR = [features[ft_idx.index(x[1])] for x in sorted(LR_per_ft, key=lambda x: x[0], reverse = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_z = []\n",
    "z = add_feature(best_ftures_LR[0], best_ftures_LR[1])\n",
    "LR_z.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,19):\n",
    "    z = add_feature(z, best_ftures_LR[i])\n",
    "    LR_z.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_z_scaled =[]\n",
    "for i in LR_z:\n",
    "    LR_z_scaled.append(MAScaler.fit_transform(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Results_combined_LR = []\n",
    "for i in LR_z:\n",
    "    Results_combined_LR.append(Resultados(i, y, clfLR))\n",
    "Results_combined_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_LR_scaled = []\n",
    "for i in LR_z_scaled:\n",
    "    Results_combined_LR_scaled.append(Resultados(i, y, clfLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_LR_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ftures_SVM = [features[ft_idx.index(x[1])] for x in sorted(SVM_per_ft, key=lambda x: x[0], reverse = True)]\n",
    "\n",
    "SVM_z = []\n",
    "SVMz = add_feature(best_ftures_SVM[0], best_ftures_SVM[1])\n",
    "SVM_z.append(SVMz)\n",
    "for i in range(2,19):\n",
    "    SVMz = add_feature(SVMz, best_ftures_SVM[i])\n",
    "    SVM_z.append(SVMz)\n",
    "\n",
    "SVM_z_scaled =[]\n",
    "for i in SVM_z:\n",
    "    SVM_z_scaled.append(MAScaler.fit_transform(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_SVM = []\n",
    "for i in SVM_z:\n",
    "    Results_combined_SVM.append(Resultados(i, y, clfSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_SVM_scaled = []\n",
    "for i in SVM_z_scaled:\n",
    "    Results_combined_SVM_scaled.append(Resultados(i, y, clfLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_SVM_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ftures_RF = [features[ft_idx.index(x[1])] for x in sorted(RF_per_ft, key=lambda x: x[0], reverse = True)]\n",
    "\n",
    "RF_z = []\n",
    "RFz = add_feature(best_ftures_RF[0], best_ftures_RF[1])\n",
    "RF_z.append(RFz)\n",
    "for i in range(2,19):\n",
    "    RFz = add_feature(RFz, best_ftures_RF[i])\n",
    "    RF_z.append(RFz)\n",
    "\n",
    "RF_z_scaled =[]\n",
    "for i in RF_z:\n",
    "    RF_z_scaled.append(MAScaler.fit_transform(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_RF = []\n",
    "for i in RF_z:\n",
    "    Results_combined_RF.append(Resultados(i, y, clfRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_RF_scaled = []\n",
    "for i in RF_z_scaled:\n",
    "    Results_combined_RF_scaled.append(Resultados(i, y, clfLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_RF_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestCombination = [Results_combined_RF.index(max(Results_combined_RF))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación conjunto de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet</th>\n",
       "      <th>humor</th>\n",
       "      <th>prejudice_woman</th>\n",
       "      <th>prejudice_lgbtiq</th>\n",
       "      <th>prejudice_inmigrant_race</th>\n",
       "      <th>gordofobia</th>\n",
       "      <th>mean_prejudice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52830</td>\n",
       "      <td>-Mamá en la escuela me dicen gorda -Pobresilla...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78883</td>\n",
       "      <td>No te sientas diferente, da igual si eres negr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78926</td>\n",
       "      <td>Si esta asi.. SUPER SI.. y que se pongan celos...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61844</td>\n",
       "      <td>—Bebé ¿Me veo gorda con este vestido?\\n—¡No mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78830</td>\n",
       "      <td>Las mujeres solo desean 2 cosas en la vida: co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              tweet  humor  \\\n",
       "0  52830  -Mamá en la escuela me dicen gorda -Pobresilla...      1   \n",
       "1  78883  No te sientas diferente, da igual si eres negr...      1   \n",
       "2  78926  Si esta asi.. SUPER SI.. y que se pongan celos...      1   \n",
       "3  61844  —Bebé ¿Me veo gorda con este vestido?\\n—¡No mi...      1   \n",
       "4  78830  Las mujeres solo desean 2 cosas en la vida: co...      1   \n",
       "\n",
       "   prejudice_woman  prejudice_lgbtiq  prejudice_inmigrant_race  gordofobia  \\\n",
       "0                1                 0                         0           1   \n",
       "1                0                 0                         1           1   \n",
       "2                1                 0                         0           1   \n",
       "3                1                 0                         0           1   \n",
       "4                1                 0                         0           1   \n",
       "\n",
       "   mean_prejudice  \n",
       "0             1.6  \n",
       "1             1.4  \n",
       "2             1.3  \n",
       "3             2.3  \n",
       "4             2.4  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('test_gold.csv')\n",
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_per_ft_test = []\n",
    "for i in range(len(features)):\n",
    "    LR_per_ft_test.append((Resultados(features[i], y_test, clfLR), ft_idx[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(LR_per_ft_test, key=lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_per_ft_test = []\n",
    "for i in range(len(features)):\n",
    "    SVM_per_ft_test.append((Resultados(features[i], y_test, clfSVC), ft_idx[i]))\n",
    "sorted(SVM_per_ft, key=lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_per_ft = []\n",
    "for i in range(len(features)):\n",
    "    RF_per_ft.append((Resultados(features[i], y_test, clfRF), ft_idx[i]))\n",
    "sorted(RF_per_ft, key=lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados combinando las características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ftures_LR_test = [features_test[ft_idx.index(x[1])] for x in sorted(LR_per_ft, key=lambda x: x[0], reverse = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_z_test = []\n",
    "z = add_feature(best_ftures_LR_test[0], best_ftures_LR_test[1])\n",
    "LR_z_test.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,19):\n",
    "    z = add_feature(z, best_ftures_LR_test[i])\n",
    "    LR_z_test.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Results_combined_LR_z = []\n",
    "for i in LR_z_test:\n",
    "    Results_combined_LR_z.append(Resultados(i, y_test, clfLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_LR_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resultados(LR_z_test, y_test, clfLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ftures_SVM_test = [features_test[ft_idx.index(x[1])] for x in sorted(SVM_per_ft, key=lambda x: x[0], reverse = True)]\n",
    "\n",
    "SVM_z_test = []\n",
    "z = add_feature(best_ftures_SVM_test[0], best_ftures_SVM_test[1])\n",
    "SVM_z_test.append(z)\n",
    "for i in range(2,19):\n",
    "    z = add_feature(z, best_ftures_SVM_test[i])\n",
    "    SVM_z_test.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_SVM_test = []\n",
    "for i in SVM_z_test:\n",
    "    Results_combined_SVM_test.append(Resultados(i, y_test, clfSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_SVM_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ftures_RF_test = [features_test[ft_idx.index(x[1])] for x in sorted(RF_per_ft, key=lambda x: x[0], reverse = True)]\n",
    "\n",
    "RF_z_test = []\n",
    "z = add_feature(best_ftures_RF_test[0], best_ftures_RF_test[1])\n",
    "RF_z_test.append(z)\n",
    "for i in range(2,19):\n",
    "    z = add_feature(z, best_ftures_RF_test[i])\n",
    "    RF_z_test.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_RF_test = []\n",
    "for i in RF_z_test:\n",
    "    Results_combined_RF_test.append(Resultados(i, y_test, clfRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_combined_RF_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = add_feature(X_bow_vect_test, X_tfidfbow_vect_test)\n",
    "w_00 = add_feature(w_0, bigram_POS_test)\n",
    "w_000 = add_feature(w_00, bigram_tfidfPOS_test)\n",
    "w_1 = add_feature(w_000, lendoc_test)\n",
    "w_2 = add_feature(w_1, palsdoc_test)\n",
    "w_3 = add_feature(w_2, speccharcnt_test)\n",
    "w_4 = add_feature(w_3, flskGL_test)\n",
    "w_5 = add_feature(w_4, emojiscnt_test)\n",
    "w_6 = add_feature(w_5, ADJcnt_test)\n",
    "w_7 = add_feature(w_6, VERBcnt_test)\n",
    "w_8 = add_feature(w_7, charspal_test)\n",
    "w_9 = add_feature(w_8, testrichlex)\n",
    "\n",
    "#X_bow_vect\n",
    "#bigram_POS\n",
    "#lendoc\n",
    "#palsdoc\n",
    "#speccharcnt\n",
    "#flskGL\n",
    "#emojiscnt\n",
    "#ADJcnt\n",
    "#VERBcnt\n",
    "#charspal\n",
    "#LexRich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultadosTarea1 = pd.DataFrame(data_test['index'])\n",
    "ResultadosTarea1['PrediccionesT1'] = pd.Series(RF_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pred = clfLR.predict(w_9)\n",
    "SVC_pred = clfSVC.predict(w_9)\n",
    "RF_pred = clfRF.predict(w_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultadosTarea1 = pd.DataFrame(data_test['index'])\n",
    "ResultadosTarea1['PrediccionesT1'] = pd.Series(RF_pred)\n",
    "ResultadosTarea1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultadosTarea1_2 = pd.DataFrame(data_test['index'])\n",
    "ResultadosTarea1_2['PrediccionesT1'] = pd.Series(SVC_pred)\n",
    "ResultadosTarea1_2.to_csv('ResultadosTarea1_2.csv', sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2A:\n",
    "\n",
    "Prejudice Target Detection:\n",
    "\n",
    "Taking into account the minority groups analyzed, i.e, Women and feminists, LGBTIQ community and Immigrants, racially discriminated people, and overweight people,  participants are asked to identify the targeted groups on each tweet as a multilabel classification task.\n",
    "\n",
    "The metric employed for the second task will be macro-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'mejor combinacion'\n",
    "z_8 = RF_z[Results_combined_RF.index(max(Results_combined_RF))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prejudice = data_train[['prejudice_woman', 'prejudice_lgbtiq', 'prejudice_inmigrant_race', 'gordofobia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( z_8, y_prejudice, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier, x_train, y_train, x_test, y_test):\n",
    "    # Make it an Multilabel classifier\n",
    "    multilabel_classifier = MultiOutputClassifier(classifier, n_jobs=-1)\n",
    "\n",
    "    # Fit the data to the Multilabel classifier\n",
    "    multilabel_classifier = multilabel_classifier.fit(x_train, y_train)\n",
    "\n",
    "    # Get predictions for test data\n",
    "    y_test_pred = multilabel_classifier.predict(x_test)\n",
    "\n",
    "    # Generate multiclass confusion matrices\n",
    "    matrices = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Plotting matrices: code\n",
    "    #cmd = ConfusionMatrixDisplay(matrices[0], display_labels=np.unique(y_test)).plot()\n",
    "    #plt.title('Confusion Matrix for label 1 (type)')\n",
    "    #plt.show()\n",
    "    #cmd = ConfusionMatrixDisplay(matrices[1], display_labels=np.unique(y_test)).plot()\n",
    "    #plt.title('Confusion Matrix for label 2 (color)')\n",
    "    #plt.show()\n",
    "\n",
    "    print(f1_score(y_test_pred, y_test, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clfLR, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clfSVC, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_classifier(SVC(C = 1, kernel = 'linear', class_weight = 'balanced'), x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clfRF, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación en el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prejudice_test = data_test[['prejudice_woman', 'prejudice_lgbtiq', 'prejudice_inmigrant_race', 'gordofobia']]\n",
    "y_prejudice_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_9 = RF_z_test[Results_combined_RF.index(max(Results_combined_RF))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_classifier = MultiOutputClassifier(clfLR, n_jobs=-1)\n",
    "multilabel_classifier = multilabel_classifier.fit(x_train, y_train)\n",
    "# Get predictions for test data\n",
    "y_pred_tarea2 = multilabel_classifier.predict(w_9)\n",
    "f1_score(y_pred_tarea2, y_prejudice_test, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_classifier = MultiOutputClassifier(SVC(C = 1, kernel = 'linear', class_weight = 'balanced'), n_jobs=-1)\n",
    "multilabel_classifier = multilabel_classifier.fit(x_train, y_train)\n",
    "# Get predictions for test data\n",
    "y_pred_tarea2 = multilabel_classifier.predict(w_9)\n",
    "f1_score(y_pred_tarea2, y_prejudice_test, average = 'macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_classifier = MultiOutputClassifier(clfRF, n_jobs=-1)\n",
    "multilabel_classifier = multilabel_classifier.fit(x_train, y_train)\n",
    "# Get predictions for test data\n",
    "y_pred_tarea2 = multilabel_classifier.predict(w_9)\n",
    "f1_score(y_pred_tarea2, y_prejudice_test, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resultados_tarea2 = pd.DataFrame(y_pred_tarea2, columns = ['prejudice_woman', 'prejudice_lgbtiq', 'prejudice_inmigrant_race', 'gordofobia'])\n",
    "Resultados_tarea2.to_csv('ResultadosTarea2.csv', sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2B:\n",
    "\n",
    "Degree of Prejudice Prediction:\n",
    "\n",
    "The third subtask consists of predicting on a continuous scale from 1 to 5 to evaluate how prejudicial the message is on average among minority groups. We will evaluate the submitted predictions employing the Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurtlex = pd.read_csv('hurtlex.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THlx = []\n",
    "for i in range(len(hurtlex)):\n",
    "    if len(hurtlex.lemma[i].split()) < 2:\n",
    "        THlx.append((hurtlex.lemma[i], hurtlex.category[i]))\n",
    "ExHlx_2 = []\n",
    "for i in range(len(hurtlex)):\n",
    "    if len(hurtlex.lemma[i].split()) == 2:\n",
    "        ExHlx_2.append((hurtlex.lemma[i], hurtlex.category[i]))\n",
    "ExHlx_3 = []\n",
    "for i in range(len(hurtlex)):\n",
    "    if len(hurtlex.lemma[i].split()) == 3:\n",
    "        ExHlx_3.append((hurtlex.lemma[i], hurtlex.category[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SHARE.txt', 'r', encoding = 'utf-8') as my_file:\n",
    "    s = my_file.readlines()\n",
    "TOf_S = []\n",
    "ExOf_S_2 = []\n",
    "ExOf_S_3 = []\n",
    "for i in s:\n",
    "    if len(i.split()) < 2:\n",
    "        TOf_S.append(i.replace('\\n', ''))\n",
    "    elif len(i.split()) == 2:\n",
    "        ExOf_S_2.append(i.replace('\\n', ''))\n",
    "    else:\n",
    "        ExOf_S_3.append(i.replace('\\n', ''))\n",
    "TOf_S.append('bastardo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('palabras_nuevas.txt', 'r', encoding = 'utf-8') as my_file:\n",
    "    palabras_nuevas = my_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_nuevas = [i.replace('\\n', '') for i in palabras_nuevas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOf_S = TOf_S + palabras_nuevas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THlx_unicos = list(set([THlx[i][0] for i in range(len(THlx))]))\n",
    "ExHlx_2_unicos = list(set([ExHlx_2[i][0] for i in range(len(ExHlx_2))]))\n",
    "ExHlx_3_unicos = list(set([ExHlx_3[i][0] for i in range(len(ExHlx_3))]))\n",
    "PalAgresivas = [pal for pal in THlx_unicos if pal not in TOf_S] + TOf_S\n",
    "ExAgres_2 = [pal for pal in ExHlx_2_unicos if pal not in ExOf_S_2] + ExOf_S_2\n",
    "ExAgres_3 = [pal for pal in ExHlx_3_unicos if pal not in ExOf_S_3] + ExOf_S_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NGramas(listaPalabras, n):\n",
    "    return [listaPalabras[i:i+n] for i in range(len(listaPalabras)-(n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palAgresivas_TotPal = []\n",
    "exprAgresivas2_totexpr2 = []\n",
    "exprAgresivas3_totexpr3 = []\n",
    "for i in data_train.tweet:\n",
    "    raw = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', i)\n",
    "    raw = emoji.replace_emoji(raw, replace = '').lower().strip()\n",
    "    raw = re.sub('_', ' ', raw.lower().strip())\n",
    "    raw = re.sub('\\n', ' ', raw.lower().strip())\n",
    "    raw = re.sub(' {2,}', ' ', raw.lower().strip())\n",
    "    ofterm = 0\n",
    "    no_ofterm = 0\n",
    "    of_bigrams = 0\n",
    "    no_ofbigrams = 0\n",
    "    of_trigrams = 0\n",
    "    no_oftrigrams = 0\n",
    "    doc = nlp(raw)\n",
    "    tot_pals = len([token.text for token in doc])\n",
    "    doc_bigrams = [' '.join(j) for j in NGramas([token.text for token in doc],2)]\n",
    "    tot_bigrams = len(doc_bigrams)\n",
    "    doc_trigrams = [' '.join(j) for j in NGramas([token.text for token in doc],3)]\n",
    "    tot_trigrams = len(doc_trigrams)\n",
    "    if tot_pals != 0:\n",
    "        for token in doc:\n",
    "            if token.text in PalAgresivas:\n",
    "                ofterm += 1\n",
    "            else:\n",
    "                no_ofterm += 1\n",
    "        palAgresivas_TotPal.append(ofterm/tot_pals)\n",
    "    else:\n",
    "        palAgresivas_TotPal.append(0)\n",
    "    if tot_bigrams != 0:\n",
    "        for bigram in doc_bigrams:\n",
    "            if bigram in ExAgres_2:\n",
    "                of_bigrams += 1\n",
    "            else:\n",
    "                no_ofbigrams += 1\n",
    "        exprAgresivas2_totexpr2.append(of_bigrams/tot_bigrams)\n",
    "    else:\n",
    "        exprAgresivas2_totexpr2.append(0)\n",
    "    if tot_trigrams != 0:\n",
    "        for trigram in doc_trigrams:\n",
    "            if trigram in ExAgres_3:\n",
    "                of_trigrams += 1\n",
    "            else:\n",
    "                no_oftrigrams += 1\n",
    "        exprAgresivas3_totexpr3.append(of_trigrams/tot_trigrams)\n",
    "    else:\n",
    "        exprAgresivas3_totexpr3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in exprAgresivas3_totexpr3 if i > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_8 El original despues sumas los indices agresivos\n",
    "z_9 = add_feature(z_8, np.array(palAgresivas_TotPal).reshape(-1, 1))\n",
    "#z_10 = add_feature(z_8, np.array(exprAgresivas2_totexpr2).reshape(-1, 1))\n",
    "#z_11 = add_feature(z_8, np.array(exprAgresivas3_totexpr3).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_meanprejudice = data_train.mean_prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfRidge = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1], cv = 10)\n",
    "clfLasso = LassoCV( cv = 2)\n",
    "clfSVM = SVR(C=1.0, epsilon=0.2, gamma= 'scale', kernel= 'linear')\n",
    "clfDTR = DecisionTreeRegressor( random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfRidge = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1], cv = 10)\n",
    "clfLasso = LassoCV( cv = 2)\n",
    "clfSVM = SVR(C=1.0, epsilon=0.2, gamma= 'scale', kernel= 'linear')\n",
    "clfSGDR = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "clfDTR = DecisionTreeRegressor( random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResTarea3(clf, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    # prediction\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # accuracy check\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse**(0.5)\n",
    "    print(\"MSE: %.2f\" % mse)\n",
    "    print(\"RMSE: %.2f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( z_8, y_meanprejudice, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ResTarea3(clfRidge, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfLasso, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfSVM, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfSGDR, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfDTR, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( z_9, y_meanprejudice, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ResTarea3(clfRidge, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfLasso, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfSVM, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfSGDR, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfDTR, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palAgresivas_TotPal_test = []\n",
    "exprAgresivas2_totexpr2_test = []\n",
    "exprAgresivas3_totexpr3_test = []\n",
    "for i in data_test.tweet:\n",
    "    raw = re.sub('[^\\w\\s]|\\d|(MENTION)|(URL)|(HASHTAG)|', '', i)\n",
    "    raw = emoji.replace_emoji(raw, replace = '').lower().strip()\n",
    "    raw = re.sub('_', ' ', raw.lower().strip())\n",
    "    raw = re.sub('\\n', ' ', raw.lower().strip())\n",
    "    raw = re.sub(' {2,}', ' ', raw.lower().strip())\n",
    "    ofterm = 0\n",
    "    no_ofterm = 0\n",
    "    of_bigrams = 0\n",
    "    no_ofbigrams = 0\n",
    "    of_trigrams = 0\n",
    "    no_oftrigrams = 0\n",
    "    doc = nlp(raw)\n",
    "    tot_pals = len([token.text for token in doc])\n",
    "    doc_bigrams = [' '.join(j) for j in NGramas([token.text for token in doc],2)]\n",
    "    tot_bigrams = len(doc_bigrams)\n",
    "    doc_trigrams = [' '.join(j) for j in NGramas([token.text for token in doc],3)]\n",
    "    tot_trigrams = len(doc_trigrams)\n",
    "    if tot_pals != 0:\n",
    "        for token in doc:\n",
    "            if token.text in PalAgresivas:\n",
    "                ofterm += 1\n",
    "            else:\n",
    "                no_ofterm += 1\n",
    "        palAgresivas_TotPal_test.append(ofterm/tot_pals)\n",
    "    else:\n",
    "        palAgresivas_TotPal_test.append(0)\n",
    "    if tot_bigrams != 0:\n",
    "        for bigram in doc_bigrams:\n",
    "            if bigram in ExAgres_2:\n",
    "                of_bigrams += 1\n",
    "            else:\n",
    "                no_ofbigrams += 1\n",
    "        exprAgresivas2_totexpr2_test.append(of_bigrams/tot_bigrams)\n",
    "    else:\n",
    "        exprAgresivas2_totexpr2_test.append(0)\n",
    "    if tot_trigrams != 0:\n",
    "        for trigram in doc_trigrams:\n",
    "            if trigram in ExAgres_3:\n",
    "                of_trigrams += 1\n",
    "            else:\n",
    "                no_oftrigrams += 1\n",
    "        exprAgresivas3_totexpr3_test.append(of_trigrams/tot_trigrams)\n",
    "    else:\n",
    "        exprAgresivas3_totexpr3_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_10 = add_feature(w_9, np.array(palAgresivas_TotPal_test).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_meanprejudice_test = data_test.mean_prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( w_10, y_meanprejudice_test, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ResTarea3(clfRidge, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfLasso, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfSVM, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfSGDR, x_train, y_train, x_test, y_test)\n",
    "ResTarea3(clfDTR, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinando Embeddings + características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_gold['clean'] = [simple_preprocess(preprocess(doc)) for doc in df_test_gold['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = [simple_preprocess(preprocess(doc)) for doc in df['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 1 for positive sentiment, 0 for negative\n",
    "x_train, x_test, y_train, y_test = train_test_split( df['clean'], df['humor'], test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "##fasttext.util.download_model('es', if_exists='ignore')  # Spanish\n",
    "ft = fasttext.load_model('cc.es.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 300\n",
    "\n",
    "model = ft\n",
    "\n",
    "def get_embedding(word):\n",
    "    try:\n",
    "        embedding = model[word]\n",
    "    except:\n",
    "        embedding = np.zeros((n_dim,))\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_embeddings =[np.mean(np.array(list(map(get_embedding,tok_sent))),axis=0) for tok_sent in x_train]\n",
    "X_test_embeddings = [np.mean(np.array(list(map(get_embedding, tok_sent))),axis=0) for tok_sent in x_test]\n",
    "X_train_embeddings =[np.mean(np.array(list(map(get_embedding, tok_sent))), axis=0) for tok_sent in x_train]\n",
    "\n",
    "X_test_gold_embeddings =[np.mean(np.array(list(map(get_embedding, tok_sent))), axis=0) for tok_sent in df_test_gold.clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stop words\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "def preprocess( doc):\n",
    "    doc = re.sub('\\W+',' ', doc.lower())\n",
    "    doc = doc.replace('\\n','')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_gold['clean'] = [simple_preprocess(preprocess(doc)) for doc in df_test_gold['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = [simple_preprocess(preprocess(doc)) for doc in df['tweet']]\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
